{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-22T13:29:37.443531Z",
     "start_time": "2025-06-22T13:29:37.409230Z"
    }
   },
   "source": [
    "from match_hybrid_embeddings import match_hybrid_embeddings\n",
    "\n",
    "match_hybrid_embeddings(\n",
    "    queries_path=r\"C:\\Users\\Reem Darawsheh\\Desktop\\PythonProject/data/hybrid_queries.joblib\",\n",
    "    hybrid_chunks_dir=r\"C:\\Users\\Reem Darawsheh\\Desktop\\PythonProject/data/hybrid_chunks\",\n",
    "    output_path=r\"C:\\Users\\Reem Darawsheh\\Desktop\\PythonProject/results/hybrid_results.json\",\n",
    "    top_k=100,\n",
    "    alpha=0.5\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T13:42:45.464484Z",
     "start_time": "2025-06-22T13:29:41.359322Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ØªÙ†ÙÙŠØ°\n",
    "if __name__ == \"__main__\":\n",
    "    queries_path = r\"C:\\Users\\Reem Darawsheh\\Desktop\\PythonProject\\PythonProject\\Query Processing\\hybridQuery\\BEIR\\quora\\test\\hybird_query_data.joblib\"\n",
    "    hybrid_chunks_dir = r\"C:\\Users\\Reem Darawsheh\\Desktop\\PythonProject\\PythonProject\\Data Representation\\Hybrid\\beir\\quora\\test\\chunks\"\n",
    "    output_path = r\"C:\\Users\\Reem Darawsheh\\Desktop\\PythonProject\\PythonProject\\Query Matching & Ranking\\HybridMatching\\hybrid_results.json\"\n",
    "\n",
    "    match_hybrid_embeddings(\n",
    "        queries_path=queries_path,\n",
    "        hybrid_chunks_dir=hybrid_chunks_dir,\n",
    "        output_path=output_path,\n",
    "        top_k=100,\n",
    "        alpha=0.5\n",
    "    )\n"
   ],
   "id": "6af88d12c2543c11",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¹ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ù‡Ø¬ÙŠÙ†Ø©...\n",
      "ğŸ“Š Ø¹Ø¯Ø¯ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª: 10000, Ø­Ø¬Ù… Ù…ÙØ±Ø¯Ø§Øª TF-IDF: 102029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ§© Ù…Ø¹Ø§Ù„Ø¬Ø© Chunks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [07:23<00:00,  4.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ: C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\Query Matching & Ranking\\HybridMatching\\hybrid_results.json\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T13:19:52.634903Z",
     "start_time": "2025-06-22T13:19:50.946592Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import joblib\n",
    "\n",
    "queries_data = joblib.load(r\"C:\\Users\\Reem Darawsheh\\Desktop\\PythonProject\\PythonProject\\Query Processing\\hybridQuery\\BEIR\\quora\\test\\hybird_query_data.joblib\")\n",
    "print(queries_data.keys())\n"
   ],
   "id": "6f41375e5a01e3d7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['query_ids', 'original_texts', 'clean_texts', 'bert_embeddings', 'tfidf_indices', 'tfidf_values', 'bert_model_name'])\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T12:05:24.584175Z",
     "start_time": "2025-06-22T12:05:20.339234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import joblib\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø£ØµÙ„ÙŠØ© Ù„Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª\n",
    "queries_data = joblib.load(r\"C:\\Users\\Reem Darawsheh\\Desktop\\PythonProject\\PythonProject\\Query Processing\\tfidf\\queries_tfidf.joblib\")\n",
    "query_ids = queries_data[\"query_ids\"]\n",
    "clean_texts = queries_data[\"clean_texts\"]\n",
    "\n",
    "# ØªØ­Ù…ÙŠÙ„ vectorizer Ø§Ù„Ø®Ø§Øµ Ø¨Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚\n",
    "doc_vectorizer_data = joblib.load(r\"C:\\Users\\Reem Darawsheh\\Desktop\\PythonProject\\PythonProject\\Data Representation\\TF-IDF\\beir\\quora\\test\\doc\\tfidf_data.joblib\")\n",
    "vectorizer = doc_vectorizer_data[\"vectorizer\"]\n",
    "\n",
    "# Ø¥Ø¹Ø§Ø¯Ø© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ù„Ù†ÙØ³ ÙØ¶Ø§Ø¡ Ø§Ù„Ù…ÙŠØ²Ø§Øª\n",
    "query_matrix = vectorizer.transform(clean_texts)\n",
    "\n",
    "# Ø­ÙØ¸ Ø§Ù„ØªÙ…Ø«ÙŠÙ„ Ø§Ù„Ù…Ø¹Ø§Ø¯ ØªØ±Ù…ÙŠØ²Ù‡\n",
    "joblib.dump({\n",
    "    \"query_ids\": query_ids,\n",
    "    \"clean_texts\": clean_texts,\n",
    "    \"query_tfidf_matrix\": query_matrix,\n",
    "    \"vectorizer\": vectorizer  # Ø§Ø®ØªÙŠØ§Ø±ÙŠ Ù„Ù„ØªÙˆØ«ÙŠÙ‚\n",
    "}, \"hybird_query_data_reencoded.joblib\")\n",
    "\n",
    "print(\"âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø¨ØªÙ…Ø«ÙŠÙ„ Ù…ØªÙˆØ§ÙÙ‚ Ù…Ø¹ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚.\")\n"
   ],
   "id": "bb73cf8d955d43af",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø¨ØªÙ…Ø«ÙŠÙ„ Ù…ØªÙˆØ§ÙÙ‚ Ù…Ø¹ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T10:16:59.307395Z",
     "start_time": "2025-06-24T10:16:59.277661Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import joblib\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import csr_matrix\n",
    "import json\n",
    "\n",
    "def match_hybrid_embeddings(\n",
    "    queries_path: str,\n",
    "    hybrid_chunks_dir: str,\n",
    "    output_path: str,\n",
    "    top_k: int = 100,\n",
    "    alpha: float = 0.5\n",
    "):\n",
    "    print(\"ğŸ”¹ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ù‡Ø¬ÙŠÙ†Ø©...\")\n",
    "    queries_data = joblib.load(queries_path)\n",
    "\n",
    "    query_ids = queries_data[\"query_ids\"]\n",
    "    tfidf_indices_list = queries_data[\"tfidf_indices\"]\n",
    "    tfidf_values_list = queries_data[\"tfidf_values\"]\n",
    "    bert_queries = np.array(queries_data[\"bert_embeddings\"], dtype=np.float32)\n",
    "\n",
    "    num_queries = len(query_ids)\n",
    "\n",
    "    # ğŸ”§ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø­Ø¬Ù… Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª Ù…Ù† Ø£ÙˆÙ„ Ù…Ù„Ù chunk\n",
    "    chunk_files = sorted([f for f in os.listdir(hybrid_chunks_dir) if f.endswith(\".joblib\")])\n",
    "    if not chunk_files:\n",
    "        raise FileNotFoundError(\"âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙŠ Ù…Ù„ÙØ§Øª joblib Ø¯Ø§Ø®Ù„ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù€ chunks.\")\n",
    "\n",
    "    first_chunk_path = os.path.join(hybrid_chunks_dir, chunk_files[0])\n",
    "    first_chunk_data = joblib.load(first_chunk_path)\n",
    "    vocab_size = first_chunk_data[\"tfidf_chunk\"].shape[1]\n",
    "\n",
    "    # Ø¥Ø¹Ø§Ø¯Ø© Ø¨Ù†Ø§Ø¡ Ù…ØµÙÙˆÙØ© TF-IDF Ù…Ù† Ø§Ù„Ù‚ÙˆØ§Ø¦Ù…\n",
    "    indptr = [0]\n",
    "    indices = []\n",
    "    data = []\n",
    "\n",
    "    for i in range(num_queries):\n",
    "        indices.extend(tfidf_indices_list[i])\n",
    "        data.extend(tfidf_values_list[i])\n",
    "        indptr.append(len(indices))\n",
    "\n",
    "    tfidf_query_matrix = csr_matrix((data, indices, indptr), shape=(num_queries, vocab_size))\n",
    "\n",
    "    print(f\"ğŸ“Š Ø¹Ø¯Ø¯ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª: {num_queries}, Ø­Ø¬Ù… Ù…ÙØ±Ø¯Ø§Øª TF-IDF: {tfidf_query_matrix.shape[1]}\")\n",
    "\n",
    "    results = {qid: [] for qid in query_ids}\n",
    "\n",
    "    for chunk_file in tqdm(chunk_files, desc=\"ğŸ§© Ù…Ø¹Ø§Ù„Ø¬Ø© Chunks\"):\n",
    "        chunk_path = os.path.join(hybrid_chunks_dir, chunk_file)\n",
    "        chunk_data = joblib.load(chunk_path)\n",
    "\n",
    "        tfidf_docs = chunk_data[\"tfidf_chunk\"]       # sparse matrix\n",
    "        bert_docs = np.array(chunk_data[\"bert_chunk\"], dtype=np.float32)\n",
    "        doc_ids = chunk_data[\"doc_ids\"]\n",
    "\n",
    "        if tfidf_query_matrix.shape[1] != tfidf_docs.shape[1]:\n",
    "            raise ValueError(\n",
    "                f\"âŒ Ø¹Ø¯Ù… ØªØ·Ø§Ø¨Ù‚ Ø¨ÙŠÙ† Ø£Ø¨Ø¹Ø§Ø¯ TF-IDF Ù„Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª ({tfidf_query_matrix.shape[1]}) \"\n",
    "                f\"ÙˆØ§Ù„ÙˆØ«Ø§Ø¦Ù‚ ({tfidf_docs.shape[1]}) ÙÙŠ {chunk_file}\"\n",
    "            )\n",
    "\n",
    "        sim_tfidf = cosine_similarity(tfidf_query_matrix, tfidf_docs)\n",
    "        sim_bert = cosine_similarity(bert_queries, bert_docs)\n",
    "        sim_hybrid = alpha * sim_tfidf + (1 - alpha) * sim_bert\n",
    "\n",
    "        for i, qid in enumerate(query_ids):\n",
    "            sims = sim_hybrid[i]\n",
    "            top_indices = np.argpartition(sims, -top_k)[-top_k:]\n",
    "            top_scores = sims[top_indices]\n",
    "            sorted_idx = top_indices[np.argsort(-top_scores)]\n",
    "\n",
    "            results[qid].extend([(doc_ids[idx], float(sims[idx])) for idx in sorted_idx])\n",
    "\n",
    "    # ØªØ±ØªÙŠØ¨ ÙˆØ£Ø®Ø° Ø£Ø¹Ù„Ù‰ top_k Ù„ÙƒÙ„ Ø§Ø³ØªØ¹Ù„Ø§Ù…\n",
    "    final_results = {}\n",
    "    for qid, docs in results.items():\n",
    "        docs.sort(key=lambda x: -x[1])\n",
    "        final_results[qid] = docs[:top_k]\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(final_results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ: {output_path}\")\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "a3be4317d3de18e3",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T13:14:08.071374Z",
     "start_time": "2025-06-24T13:13:47.823747Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ØªÙ†ÙÙŠØ°\n",
    "if __name__ == \"__main__\":\n",
    "    queries_path = r\"C:\\Users\\Reem Darawsheh\\Desktop\\PythonProject\\PythonProject\\Query Processing\\hybridQuery\\Antique\\train\\hybird_query_data.joblib\"\n",
    "    hybrid_chunks_dir = r\"C:\\Users\\Reem Darawsheh\\Desktop\\PythonProject\\PythonProject\\Data Representation\\Hybrid\\antique\\train\\chunks\"\n",
    "    output_path = r\"C:\\Users\\Reem Darawsheh\\Desktop\\PythonProject\\PythonProject\\Query Matching & Ranking\\HybridMatching\\hybrid_results_antique.json\"\n",
    "\n",
    "    match_hybrid_embeddings(\n",
    "        queries_path=queries_path,\n",
    "        hybrid_chunks_dir=hybrid_chunks_dir,\n",
    "        output_path=output_path,\n",
    "        top_k=100,\n",
    "        alpha=0.5\n",
    "    )\n"
   ],
   "id": "ebb006b02e6d5d63",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¹ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ù‡Ø¬ÙŠÙ†Ø©...\n",
      "ğŸ“Š Ø¹Ø¯Ø¯ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª: 176, Ø­Ø¬Ù… Ù…ÙØ±Ø¯Ø§Øª TF-IDF: 250274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ§© Ù…Ø¹Ø§Ù„Ø¬Ø© Chunks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81/81 [00:18<00:00,  4.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ: C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\Query Matching & Ranking\\HybridMatching\\hybrid_results_antique.json\n"
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
