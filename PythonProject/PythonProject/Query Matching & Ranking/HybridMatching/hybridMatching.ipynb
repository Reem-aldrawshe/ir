{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-22T13:29:37.443531Z",
     "start_time": "2025-06-22T13:29:37.409230Z"
    }
   },
   "source": [
    "from match_hybrid_embeddings import match_hybrid_embeddings\n",
    "\n",
    "match_hybrid_embeddings(\n",
    "    queries_path=r\"C:\\Users\\Reem Darawsheh\\Desktop\\PythonProject/data/hybrid_queries.joblib\",\n",
    "    hybrid_chunks_dir=r\"C:\\Users\\Reem Darawsheh\\Desktop\\PythonProject/data/hybrid_chunks\",\n",
    "    output_path=r\"C:\\Users\\Reem Darawsheh\\Desktop\\PythonProject/results/hybrid_results.json\",\n",
    "    top_k=100,\n",
    "    alpha=0.5\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T13:42:45.464484Z",
     "start_time": "2025-06-22T13:29:41.359322Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# تنفيذ\n",
    "if __name__ == \"__main__\":\n",
    "    queries_path = r\"C:\\Users\\Reem Darawsheh\\Desktop\\PythonProject\\PythonProject\\Query Processing\\hybridQuery\\BEIR\\quora\\test\\hybird_query_data.joblib\"\n",
    "    hybrid_chunks_dir = r\"C:\\Users\\Reem Darawsheh\\Desktop\\PythonProject\\PythonProject\\Data Representation\\Hybrid\\beir\\quora\\test\\chunks\"\n",
    "    output_path = r\"C:\\Users\\Reem Darawsheh\\Desktop\\PythonProject\\PythonProject\\Query Matching & Ranking\\HybridMatching\\hybrid_results.json\"\n",
    "\n",
    "    match_hybrid_embeddings(\n",
    "        queries_path=queries_path,\n",
    "        hybrid_chunks_dir=hybrid_chunks_dir,\n",
    "        output_path=output_path,\n",
    "        top_k=100,\n",
    "        alpha=0.5\n",
    "    )\n"
   ],
   "id": "6af88d12c2543c11",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 تحميل الاستعلامات الهجينة...\n",
      "📊 عدد الاستعلامات: 10000, حجم مفردات TF-IDF: 102029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🧩 معالجة Chunks: 100%|██████████| 105/105 [07:23<00:00,  4.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ تم حفظ النتائج في: C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\Query Matching & Ranking\\HybridMatching\\hybrid_results.json\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T13:19:52.634903Z",
     "start_time": "2025-06-22T13:19:50.946592Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import joblib\n",
    "\n",
    "queries_data = joblib.load(r\"C:\\Users\\Reem Darawsheh\\Desktop\\PythonProject\\PythonProject\\Query Processing\\hybridQuery\\BEIR\\quora\\test\\hybird_query_data.joblib\")\n",
    "print(queries_data.keys())\n"
   ],
   "id": "6f41375e5a01e3d7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['query_ids', 'original_texts', 'clean_texts', 'bert_embeddings', 'tfidf_indices', 'tfidf_values', 'bert_model_name'])\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T12:05:24.584175Z",
     "start_time": "2025-06-22T12:05:20.339234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import joblib\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "# تحميل النصوص الأصلية للاستعلامات\n",
    "queries_data = joblib.load(r\"C:\\Users\\Reem Darawsheh\\Desktop\\PythonProject\\PythonProject\\Query Processing\\tfidf\\queries_tfidf.joblib\")\n",
    "query_ids = queries_data[\"query_ids\"]\n",
    "clean_texts = queries_data[\"clean_texts\"]\n",
    "\n",
    "# تحميل vectorizer الخاص بالوثائق\n",
    "doc_vectorizer_data = joblib.load(r\"C:\\Users\\Reem Darawsheh\\Desktop\\PythonProject\\PythonProject\\Data Representation\\TF-IDF\\beir\\quora\\test\\doc\\tfidf_data.joblib\")\n",
    "vectorizer = doc_vectorizer_data[\"vectorizer\"]\n",
    "\n",
    "# إعادة تحويل الاستعلامات لنفس فضاء الميزات\n",
    "query_matrix = vectorizer.transform(clean_texts)\n",
    "\n",
    "# حفظ التمثيل المعاد ترميزه\n",
    "joblib.dump({\n",
    "    \"query_ids\": query_ids,\n",
    "    \"clean_texts\": clean_texts,\n",
    "    \"query_tfidf_matrix\": query_matrix,\n",
    "    \"vectorizer\": vectorizer  # اختياري للتوثيق\n",
    "}, \"hybird_query_data_reencoded.joblib\")\n",
    "\n",
    "print(\"✅ تم حفظ الاستعلامات بتمثيل متوافق مع الوثائق.\")\n"
   ],
   "id": "bb73cf8d955d43af",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ تم حفظ الاستعلامات بتمثيل متوافق مع الوثائق.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T10:16:59.307395Z",
     "start_time": "2025-06-24T10:16:59.277661Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import joblib\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import csr_matrix\n",
    "import json\n",
    "\n",
    "def match_hybrid_embeddings(\n",
    "    queries_path: str,\n",
    "    hybrid_chunks_dir: str,\n",
    "    output_path: str,\n",
    "    top_k: int = 100,\n",
    "    alpha: float = 0.5\n",
    "):\n",
    "    print(\"🔹 تحميل الاستعلامات الهجينة...\")\n",
    "    queries_data = joblib.load(queries_path)\n",
    "\n",
    "    query_ids = queries_data[\"query_ids\"]\n",
    "    tfidf_indices_list = queries_data[\"tfidf_indices\"]\n",
    "    tfidf_values_list = queries_data[\"tfidf_values\"]\n",
    "    bert_queries = np.array(queries_data[\"bert_embeddings\"], dtype=np.float32)\n",
    "\n",
    "    num_queries = len(query_ids)\n",
    "\n",
    "    # 🔧 استخراج حجم المفردات من أول ملف chunk\n",
    "    chunk_files = sorted([f for f in os.listdir(hybrid_chunks_dir) if f.endswith(\".joblib\")])\n",
    "    if not chunk_files:\n",
    "        raise FileNotFoundError(\"❌ لم يتم العثور على أي ملفات joblib داخل مجلد الـ chunks.\")\n",
    "\n",
    "    first_chunk_path = os.path.join(hybrid_chunks_dir, chunk_files[0])\n",
    "    first_chunk_data = joblib.load(first_chunk_path)\n",
    "    vocab_size = first_chunk_data[\"tfidf_chunk\"].shape[1]\n",
    "\n",
    "    # إعادة بناء مصفوفة TF-IDF من القوائم\n",
    "    indptr = [0]\n",
    "    indices = []\n",
    "    data = []\n",
    "\n",
    "    for i in range(num_queries):\n",
    "        indices.extend(tfidf_indices_list[i])\n",
    "        data.extend(tfidf_values_list[i])\n",
    "        indptr.append(len(indices))\n",
    "\n",
    "    tfidf_query_matrix = csr_matrix((data, indices, indptr), shape=(num_queries, vocab_size))\n",
    "\n",
    "    print(f\"📊 عدد الاستعلامات: {num_queries}, حجم مفردات TF-IDF: {tfidf_query_matrix.shape[1]}\")\n",
    "\n",
    "    results = {qid: [] for qid in query_ids}\n",
    "\n",
    "    for chunk_file in tqdm(chunk_files, desc=\"🧩 معالجة Chunks\"):\n",
    "        chunk_path = os.path.join(hybrid_chunks_dir, chunk_file)\n",
    "        chunk_data = joblib.load(chunk_path)\n",
    "\n",
    "        tfidf_docs = chunk_data[\"tfidf_chunk\"]       # sparse matrix\n",
    "        bert_docs = np.array(chunk_data[\"bert_chunk\"], dtype=np.float32)\n",
    "        doc_ids = chunk_data[\"doc_ids\"]\n",
    "\n",
    "        if tfidf_query_matrix.shape[1] != tfidf_docs.shape[1]:\n",
    "            raise ValueError(\n",
    "                f\"❌ عدم تطابق بين أبعاد TF-IDF للاستعلامات ({tfidf_query_matrix.shape[1]}) \"\n",
    "                f\"والوثائق ({tfidf_docs.shape[1]}) في {chunk_file}\"\n",
    "            )\n",
    "\n",
    "        sim_tfidf = cosine_similarity(tfidf_query_matrix, tfidf_docs)\n",
    "        sim_bert = cosine_similarity(bert_queries, bert_docs)\n",
    "        sim_hybrid = alpha * sim_tfidf + (1 - alpha) * sim_bert\n",
    "\n",
    "        for i, qid in enumerate(query_ids):\n",
    "            sims = sim_hybrid[i]\n",
    "            top_indices = np.argpartition(sims, -top_k)[-top_k:]\n",
    "            top_scores = sims[top_indices]\n",
    "            sorted_idx = top_indices[np.argsort(-top_scores)]\n",
    "\n",
    "            results[qid].extend([(doc_ids[idx], float(sims[idx])) for idx in sorted_idx])\n",
    "\n",
    "    # ترتيب وأخذ أعلى top_k لكل استعلام\n",
    "    final_results = {}\n",
    "    for qid, docs in results.items():\n",
    "        docs.sort(key=lambda x: -x[1])\n",
    "        final_results[qid] = docs[:top_k]\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(final_results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"✅ تم حفظ النتائج في: {output_path}\")\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "a3be4317d3de18e3",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T13:14:08.071374Z",
     "start_time": "2025-06-24T13:13:47.823747Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# تنفيذ\n",
    "if __name__ == \"__main__\":\n",
    "    queries_path = r\"C:\\Users\\Reem Darawsheh\\Desktop\\PythonProject\\PythonProject\\Query Processing\\hybridQuery\\Antique\\train\\hybird_query_data.joblib\"\n",
    "    hybrid_chunks_dir = r\"C:\\Users\\Reem Darawsheh\\Desktop\\PythonProject\\PythonProject\\Data Representation\\Hybrid\\antique\\train\\chunks\"\n",
    "    output_path = r\"C:\\Users\\Reem Darawsheh\\Desktop\\PythonProject\\PythonProject\\Query Matching & Ranking\\HybridMatching\\hybrid_results_antique.json\"\n",
    "\n",
    "    match_hybrid_embeddings(\n",
    "        queries_path=queries_path,\n",
    "        hybrid_chunks_dir=hybrid_chunks_dir,\n",
    "        output_path=output_path,\n",
    "        top_k=100,\n",
    "        alpha=0.5\n",
    "    )\n"
   ],
   "id": "ebb006b02e6d5d63",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 تحميل الاستعلامات الهجينة...\n",
      "📊 عدد الاستعلامات: 176, حجم مفردات TF-IDF: 250274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🧩 معالجة Chunks: 100%|██████████| 81/81 [00:18<00:00,  4.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ تم حفظ النتائج في: C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\Query Matching & Ranking\\HybridMatching\\hybrid_results_antique.json\n"
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
