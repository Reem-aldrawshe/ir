{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T08:20:55.514923Z",
     "start_time": "2025-06-24T08:20:54.371434Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\Reem Darawsheh\\Desktop\\PythonProject/PythonProject/scripts\")  # المسار المناسب لمكان السكريبت\n",
    "\n",
    "from hybrid_representation import build_hybrid_representation_in_chunks_joblib\n",
    "\n",
    "# مثال على الاستدعاء\n",
    "build_hybrid_representation_in_chunks_joblib(\n",
    "    tfidf_path=\"antique/train/doc/tfidf.joblib\",\n",
    "    bert_path=\"antique/train/doc/bert_embedding.joblib\",\n",
    "    save_path=\"antique/train/doc/hybrid_chunks\"\n",
    ")\n"
   ],
   "id": "ca5ec02f5acb1021",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:15:54.905897Z",
     "start_time": "2025-06-19T12:13:28.551215Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "build_hybrid_representation_in_chunks_joblib(\n",
    "    tfidf_path=r\"C:\\Users\\Reem Darawsheh\\Desktop\\PythonProject\\PythonProject\\Data Representation\\TF-IDF\\beir\\quora\\test\\doc\\tfidf_data.joblib\",\n",
    "    bert_path=r\"C:\\Users\\Reem Darawsheh\\Desktop\\PythonProject\\PythonProject\\Data Representation\\Bert\\beir\\quora\\test\\doc\\bert_embedding.joblib\",\n",
    "    save_path=r\"C:\\Users\\Reem Darawsheh\\Desktop\\PythonProject\\PythonProject\\Data Representation\\Hybrid\\beir\\quora\\test\\chunks\",\n",
    "    chunk_size=5000)"
   ],
   "id": "d644f9ebe8a708be",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 بناء التمثيل الهجين باستخدام TF-IDF وBERT بشكل دفعات (Chunks)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🧩 Processing Chunks: 100%|██████████| 105/105 [01:30<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ تم حفظ جميع التمثيلات الهجينة بصيغة joblib في: C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\Data Representation\\Hybrid\\beir\\quora\\test\\chunks\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T08:41:11.492559Z",
     "start_time": "2025-06-24T08:41:11.461278Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def build_clean_hybrid_representation_in_chunks(\n",
    "    tfidf_path,\n",
    "    bert_path,\n",
    "    save_path,\n",
    "    chunk_size=5000\n",
    "):\n",
    "    print(\"🚀 بدء معالجة التمثيلات الهجينة مع التحقق من التداخل...\")\n",
    "\n",
    "    # 1. تحميل البيانات\n",
    "    tfidf_data = joblib.load(tfidf_path)\n",
    "    bert_data = joblib.load(bert_path)\n",
    "\n",
    "    tfidf_matrix = tfidf_data[\"tfidf_matrix\"]\n",
    "    tfidf_doc_ids = tfidf_data[\"doc_ids\"]\n",
    "\n",
    "    bert_matrix = np.array(bert_data[\"embeddings_matrix\"], dtype=np.float32)\n",
    "    bert_doc_ids = bert_data[\"doc_ids\"]\n",
    "\n",
    "    # 2. إيجاد التقاطع بين doc_ids\n",
    "    common_doc_ids = list(set(tfidf_doc_ids) & set(bert_doc_ids))\n",
    "    common_doc_ids.sort()  # ترتيب ثابت\n",
    "\n",
    "    print(f\"📌 عدد الوثائق المشتركة: {len(common_doc_ids)} / TF-IDF: {len(tfidf_doc_ids)} / BERT: {len(bert_doc_ids)}\")\n",
    "\n",
    "    # 3. بناء map من doc_id إلى index لكلا المصدرين\n",
    "    tfidf_id_to_index = {doc_id: idx for idx, doc_id in enumerate(tfidf_doc_ids)}\n",
    "    bert_id_to_index = {doc_id: idx for idx, doc_id in enumerate(bert_doc_ids)}\n",
    "\n",
    "    # 4. تصفية وتمثيل البيانات المشتركة فقط\n",
    "    filtered_tfidf = tfidf_matrix[[tfidf_id_to_index[doc_id] for doc_id in common_doc_ids]]\n",
    "    filtered_bert = np.array([bert_matrix[bert_id_to_index[doc_id]] for doc_id in common_doc_ids], dtype=np.float32)\n",
    "\n",
    "    # 5. التأكد\n",
    "    assert filtered_tfidf.shape[0] == filtered_bert.shape[0] == len(common_doc_ids)\n",
    "\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    total_docs = len(common_doc_ids)\n",
    "\n",
    "    # 6. التقطيع إلى Chunks\n",
    "    for start in tqdm(range(0, total_docs, chunk_size), desc=\"🧩 معالجة Chunks\"):\n",
    "        end = min(start + chunk_size, total_docs)\n",
    "\n",
    "        tfidf_chunk = filtered_tfidf[start:end]\n",
    "        bert_chunk = filtered_bert[start:end]\n",
    "        chunk_doc_ids = common_doc_ids[start:end]\n",
    "\n",
    "        chunk_data = {\n",
    "            \"tfidf_chunk\": tfidf_chunk,\n",
    "            \"bert_chunk\": bert_chunk,\n",
    "            \"doc_ids\": chunk_doc_ids\n",
    "        }\n",
    "\n",
    "        chunk_file = os.path.join(save_path, f\"hybrid_chunk_{start}_{end}.joblib\")\n",
    "        joblib.dump(chunk_data, chunk_file, compress=3)\n",
    "\n",
    "    print(f\"✅ تم إنشاء {total_docs} وثيقة في دفعات داخل: {save_path}\")\n",
    "\n",
    "\n"
   ],
   "id": "7d344e9dee539d9f",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T08:43:39.524268Z",
     "start_time": "2025-06-24T08:41:38.388289Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 🔸 تنفيذ\n",
    "build_clean_hybrid_representation_in_chunks(\n",
    "    tfidf_path=r\"C:\\Users\\Reem Darawsheh\\Desktop\\PythonProject\\PythonProject\\Data Representation\\TF-IDF\\antique\\train\\doc\\tfidf_data.joblib\",\n",
    "    bert_path=r\"C:\\Users\\Reem Darawsheh\\Desktop\\PythonProject\\PythonProject\\Data Representation\\Bert\\antique\\train\\doc\\bert_embedding.joblib\",\n",
    "    save_path=r\"C:\\Users\\Reem Darawsheh\\Desktop\\PythonProject\\PythonProject\\Data Representation\\Hybrid\\antique\\train\\chunks\",\n",
    "    chunk_size=5000\n",
    ")"
   ],
   "id": "1eb8d518f97935ce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 بدء معالجة التمثيلات الهجينة مع التحقق من التداخل...\n",
      "📌 عدد الوثائق المشتركة: 401768 / TF-IDF: 403666 / BERT: 401768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🧩 معالجة Chunks: 100%|██████████| 81/81 [01:09<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ تم إنشاء 401768 وثيقة في دفعات داخل: C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\Data Representation\\Hybrid\\antique\\train\\chunks\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:16:39.148482Z",
     "start_time": "2025-06-19T12:16:38.887994Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import joblib\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def validate_hybrid_chunk_joblib(chunk_start, chunk_end, base_path, tfidf_dim=100000, bert_dim=384):\n",
    "    # بناء اسم ملف chunk\n",
    "    chunk_file = os.path.join(base_path, f\"hybrid_chunk_{chunk_start}_{chunk_end}.joblib\")\n",
    "\n",
    "    if not os.path.exists(chunk_file):\n",
    "        print(f\"❌ الملف غير موجود: {chunk_file}\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n📂 Checking hybrid chunk joblib {chunk_start}-{chunk_end}\")\n",
    "\n",
    "    # تحميل البيانات\n",
    "    chunk_data = joblib.load(chunk_file)\n",
    "\n",
    "    # التأكد من وجود المفاتيح المطلوبة\n",
    "    required_keys = [\"tfidf_chunk\", \"bert_chunk\", \"doc_ids\"]\n",
    "    for key in required_keys:\n",
    "        if key not in chunk_data:\n",
    "            print(f\"❌ المفتاح '{key}' غير موجود في الملف\")\n",
    "            return\n",
    "\n",
    "    tfidf_chunk = chunk_data[\"tfidf_chunk\"]\n",
    "    bert_chunk = chunk_data[\"bert_chunk\"]\n",
    "    doc_ids = chunk_data[\"doc_ids\"]\n",
    "\n",
    "    # التحقق من الأبعاد وعدد الوثائق\n",
    "    num_docs = len(doc_ids)\n",
    "    if tfidf_chunk.shape[0] != num_docs:\n",
    "        print(f\"❌ عدد الصفوف في tfidf_chunk ({tfidf_chunk.shape[0]}) لا يطابق عدد doc_ids ({num_docs})\")\n",
    "        return\n",
    "    if bert_chunk.shape[0] != num_docs:\n",
    "        print(f\"❌ عدد الصفوف في bert_chunk ({bert_chunk.shape[0]}) لا يطابق عدد doc_ids ({num_docs})\")\n",
    "        return\n",
    "    if tfidf_chunk.shape[1] != tfidf_dim:\n",
    "        print(f\"❌ عدد أعمدة tfidf_chunk غير متوقع: {tfidf_chunk.shape[1]} != {tfidf_dim}\")\n",
    "        return\n",
    "    if bert_chunk.shape[1] != bert_dim:\n",
    "        print(f\"❌ عدد أعمدة bert_chunk غير متوقع: {bert_chunk.shape[1]} != {bert_dim}\")\n",
    "        return\n",
    "\n",
    "    print(f\"✅ Document count: {num_docs}\")\n",
    "    print(f\"✅ TF-IDF chunk shape: {tfidf_chunk.shape}\")\n",
    "    print(f\"✅ BERT chunk shape: {bert_chunk.shape}\")\n",
    "\n",
    "    # اختبار تشابه بسيط: نحسب تشابه cos بين أول وثيقتين باستخدام التمثيل الهجين (concat)\n",
    "    if num_docs >= 2:\n",
    "        # جمع التمثيل الهجين مؤقتًا (dense + sparse)\n",
    "        from scipy.sparse import hstack\n",
    "\n",
    "        hybrid_0 = np.hstack([tfidf_chunk[0].toarray().flatten(), bert_chunk[0]])\n",
    "        hybrid_1 = np.hstack([tfidf_chunk[1].toarray().flatten(), bert_chunk[1]])\n",
    "\n",
    "        sim = cosine_similarity([hybrid_0], [hybrid_1])[0][0]\n",
    "        print(f\"🧪 Cosine similarity بين أول وثيقتين في التمثيل الهجين: {sim:.4f}\")\n",
    "    else:\n",
    "        print(\"ℹ️ Chunk يحتوي على وثيقة واحدة فقط، لا يمكن حساب التشابه.\")\n",
    "\n",
    "    print(\"✅ التمثيل الهجين في chunk صحيح.\")\n",
    "\n",
    "\n",
    "# مثال على الاستخدام\n",
    "validate_hybrid_chunk_joblib(\n",
    "    chunk_start=0,\n",
    "    chunk_end=5000,\n",
    "    base_path=r\"C:\\Users\\Reem Darawsheh\\Desktop\\PythonProject\\PythonProject\\Data Representation\\Hybrid\\beir\\quora\\test\\chunks\",\n",
    "    tfidf_dim=102029,\n",
    "    bert_dim=384\n",
    ")\n",
    "\n"
   ],
   "id": "1dba04b4f58619fb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📂 Checking hybrid chunk joblib 0-5000\n",
      "✅ Document count: 5000\n",
      "✅ TF-IDF chunk shape: (5000, 102029)\n",
      "✅ BERT chunk shape: (5000, 384)\n",
      "🧪 Cosine similarity بين أول وثيقتين في التمثيل الهجين: 0.9058\n",
      "✅ التمثيل الهجين في chunk صحيح.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:17:13.379961Z",
     "start_time": "2025-06-19T12:17:13.359884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def hybrid_parallel_search(\n",
    "    query_tfidf_vector,\n",
    "    query_bert_vector,\n",
    "    chunks_path,\n",
    "    tfidf_weight=0.5,\n",
    "    bert_weight=0.5,\n",
    "    top_k=10\n",
    "):\n",
    "    \"\"\"\n",
    "    بحث تمثيل متوازي مع دمج النتائج fusion weights\n",
    "\n",
    "    - query_tfidf_vector: تمثيل الاستعلام باستخدام TF-IDF (sparse vector أو dense numpy array)\n",
    "    - query_bert_vector: تمثيل الاستعلام باستخدام BERT (dense numpy array)\n",
    "    - chunks_path: مجلد يحوي ملفات chunk بصيغة joblib مع تمثيلات TF-IDF و BERT\n",
    "    - tfidf_weight, bert_weight: أوزان دمج التشابه (يجب أن مجموعهم=1)\n",
    "    - top_k: عدد النتائج النهائية المراد إرجاعها\n",
    "    \"\"\"\n",
    "\n",
    "    assert abs(tfidf_weight + bert_weight - 1.0) < 1e-6, \"مجموع الأوزان يجب أن يساوي 1\"\n",
    "\n",
    "    final_results = []\n",
    "\n",
    "    chunk_files = sorted([f for f in os.listdir(chunks_path) if f.endswith(\".joblib\")])\n",
    "    for chunk_file in tqdm(chunk_files, desc=\"⚙️ معالجة chunks\"):\n",
    "        chunk_data = joblib.load(os.path.join(chunks_path, chunk_file))\n",
    "\n",
    "        tfidf_chunk = chunk_data[\"tfidf_chunk\"]  # sparse matrix\n",
    "        bert_chunk = chunk_data[\"bert_chunk\"]    # dense numpy array\n",
    "        doc_ids = chunk_data[\"doc_ids\"]\n",
    "\n",
    "        # حساب تشابه TF-IDF (cosine similarity)\n",
    "        sims_tfidf = cosine_similarity(query_tfidf_vector, tfidf_chunk).flatten()\n",
    "\n",
    "        # حساب تشابه BERT (cosine similarity)\n",
    "        sims_bert = cosine_similarity([query_bert_vector], bert_chunk).flatten()\n",
    "\n",
    "        # دمج التشابهات بالأوزان المحددة\n",
    "        sims_fused = tfidf_weight * sims_tfidf + bert_weight * sims_bert\n",
    "\n",
    "        # جمع النتائج مع doc_ids\n",
    "        final_results.extend(zip(doc_ids, sims_fused))\n",
    "\n",
    "    # ترتيب النتائج النهائية تنازليًا حسب التشابه\n",
    "    final_results.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_results = final_results[:top_k]\n",
    "\n",
    "    print(\"\\n✅ النتائج النهائية (Top {}):\".format(top_k))\n",
    "    for i, (doc_id, score) in enumerate(top_results, 1):\n",
    "        print(f\"{i}. {doc_id} | Score: {score:.4f}\")\n",
    "\n",
    "    return top_results\n"
   ],
   "id": "c8044b0195a249e0",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:43:46.747506Z",
     "start_time": "2025-06-19T12:43:24.791958Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "# 1. حضّر موديل BERT\n",
    "bert_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# 2. جهز الاستعلام\n",
    "query = \"how to learn a amchine programming language? \"\n",
    "\n",
    "# 3. حمّل vectorizer من ملف tfidf_data.joblib (لأنه يحتويه)\n",
    "vectorizer_data = joblib.load(r\"C:\\Users\\Reem Darawsheh\\Desktop\\PythonProject\\PythonProject\\Data Representation\\TF-IDF\\beir\\quora\\test\\doc\\tfidf_data.joblib\")\n",
    "vectorizer = vectorizer_data[\"vectorizer\"]\n",
    "\n",
    "# 4. حول الاستعلام إلى تمثيل TF-IDF\n",
    "query_tfidf_vector = vectorizer.transform([query])\n",
    "\n",
    "# 5. تمثيل الاستعلام باستخدام BERT\n",
    "query_bert_vector = bert_model.encode(query, convert_to_numpy=True).astype(np.float32)\n",
    "\n",
    "# 6. نفّذ البحث (تأكد أن تابع hybrid_parallel_search معرف ومتاح)\n",
    "results = hybrid_parallel_search(\n",
    "    query_tfidf_vector=query_tfidf_vector,\n",
    "    query_bert_vector=query_bert_vector,\n",
    "    chunks_path=r\"C:\\Users\\Reem Darawsheh\\Desktop\\PythonProject\\PythonProject\\Data Representation\\Hybrid\\beir\\quora\\test\\chunks\",\n",
    "    tfidf_weight=0.4,\n",
    "    bert_weight=0.6,\n",
    "    top_k=10\n",
    ")\n",
    "\n",
    "# 7. عرض النتائج\n",
    "print(results)\n"
   ],
   "id": "7a28d69808c56424",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "⚙️ معالجة chunks: 100%|██████████| 105/105 [00:16<00:00,  6.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ النتائج النهائية (Top 10):\n",
      "1. 91642 | Score: 0.6005\n",
      "2. 527808 | Score: 0.5991\n",
      "3. 498101 | Score: 0.5965\n",
      "4. 44161 | Score: 0.5810\n",
      "5. 1271 | Score: 0.5602\n",
      "6. 112819 | Score: 0.5481\n",
      "7. 364397 | Score: 0.5396\n",
      "8. 22650 | Score: 0.5242\n",
      "9. 9811 | Score: 0.5242\n",
      "10. 419330 | Score: 0.5234\n",
      "[('91642', np.float64(0.6005096524953842)), ('527808', np.float64(0.5991446912288666)), ('498101', np.float64(0.5965242475271225)), ('44161', np.float64(0.5810033917427063)), ('1271', np.float64(0.5602440029382706)), ('112819', np.float64(0.5480646084057077)), ('364397', np.float64(0.5395946989761575)), ('22650', np.float64(0.524203137870819)), ('9811', np.float64(0.524203137870819)), ('419330', np.float64(0.5233781973825677))]\n"
     ]
    }
   ],
   "execution_count": 21
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
