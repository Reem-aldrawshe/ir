{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T08:20:55.514923Z",
     "start_time": "2025-06-24T08:20:54.371434Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\Reem Darawsheh\\Desktop\\PythonProject/PythonProject/scripts\")  # Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ù†Ø§Ø³Ø¨ Ù„Ù…ÙƒØ§Ù† Ø§Ù„Ø³ÙƒØ±ÙŠØ¨Øª\n",
    "\n",
    "from hybrid_representation import build_hybrid_representation_in_chunks_joblib\n",
    "\n",
    "# Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ¯Ø¹Ø§Ø¡\n",
    "build_hybrid_representation_in_chunks_joblib(\n",
    "    tfidf_path=\"antique/train/doc/tfidf.joblib\",\n",
    "    bert_path=\"antique/train/doc/bert_embedding.joblib\",\n",
    "    save_path=\"antique/train/doc/hybrid_chunks\"\n",
    ")\n"
   ],
   "id": "ca5ec02f5acb1021",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:15:54.905897Z",
     "start_time": "2025-06-19T12:13:28.551215Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "build_hybrid_representation_in_chunks_joblib(\n",
    "    tfidf_path=r\"C:\\Users\\Reem Darawsheh\\Desktop\\PythonProject\\PythonProject\\Data Representation\\TF-IDF\\beir\\quora\\test\\doc\\tfidf_data.joblib\",\n",
    "    bert_path=r\"C:\\Users\\Reem Darawsheh\\Desktop\\PythonProject\\PythonProject\\Data Representation\\Bert\\beir\\quora\\test\\doc\\bert_embedding.joblib\",\n",
    "    save_path=r\"C:\\Users\\Reem Darawsheh\\Desktop\\PythonProject\\PythonProject\\Data Representation\\Hybrid\\beir\\quora\\test\\chunks\",\n",
    "    chunk_size=5000)"
   ],
   "id": "d644f9ebe8a708be",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Ø¨Ù†Ø§Ø¡ Ø§Ù„ØªÙ…Ø«ÙŠÙ„ Ø§Ù„Ù‡Ø¬ÙŠÙ† Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… TF-IDF ÙˆBERT Ø¨Ø´ÙƒÙ„ Ø¯ÙØ¹Ø§Øª (Chunks)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ§© Processing Chunks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [01:30<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ØªÙ… Ø­ÙØ¸ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªÙ…Ø«ÙŠÙ„Ø§Øª Ø§Ù„Ù‡Ø¬ÙŠÙ†Ø© Ø¨ØµÙŠØºØ© joblib ÙÙŠ: C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\Data Representation\\Hybrid\\beir\\quora\\test\\chunks\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T08:41:11.492559Z",
     "start_time": "2025-06-24T08:41:11.461278Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def build_clean_hybrid_representation_in_chunks(\n",
    "    tfidf_path,\n",
    "    bert_path,\n",
    "    save_path,\n",
    "    chunk_size=5000\n",
    "):\n",
    "    print(\"ğŸš€ Ø¨Ø¯Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØªÙ…Ø«ÙŠÙ„Ø§Øª Ø§Ù„Ù‡Ø¬ÙŠÙ†Ø© Ù…Ø¹ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØªØ¯Ø§Ø®Ù„...\")\n",
    "\n",
    "    # 1. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "    tfidf_data = joblib.load(tfidf_path)\n",
    "    bert_data = joblib.load(bert_path)\n",
    "\n",
    "    tfidf_matrix = tfidf_data[\"tfidf_matrix\"]\n",
    "    tfidf_doc_ids = tfidf_data[\"doc_ids\"]\n",
    "\n",
    "    bert_matrix = np.array(bert_data[\"embeddings_matrix\"], dtype=np.float32)\n",
    "    bert_doc_ids = bert_data[\"doc_ids\"]\n",
    "\n",
    "    # 2. Ø¥ÙŠØ¬Ø§Ø¯ Ø§Ù„ØªÙ‚Ø§Ø·Ø¹ Ø¨ÙŠÙ† doc_ids\n",
    "    common_doc_ids = list(set(tfidf_doc_ids) & set(bert_doc_ids))\n",
    "    common_doc_ids.sort()  # ØªØ±ØªÙŠØ¨ Ø«Ø§Ø¨Øª\n",
    "\n",
    "    print(f\"ğŸ“Œ Ø¹Ø¯Ø¯ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…Ø´ØªØ±ÙƒØ©: {len(common_doc_ids)} / TF-IDF: {len(tfidf_doc_ids)} / BERT: {len(bert_doc_ids)}\")\n",
    "\n",
    "    # 3. Ø¨Ù†Ø§Ø¡ map Ù…Ù† doc_id Ø¥Ù„Ù‰ index Ù„ÙƒÙ„Ø§ Ø§Ù„Ù…ØµØ¯Ø±ÙŠÙ†\n",
    "    tfidf_id_to_index = {doc_id: idx for idx, doc_id in enumerate(tfidf_doc_ids)}\n",
    "    bert_id_to_index = {doc_id: idx for idx, doc_id in enumerate(bert_doc_ids)}\n",
    "\n",
    "    # 4. ØªØµÙÙŠØ© ÙˆØªÙ…Ø«ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø´ØªØ±ÙƒØ© ÙÙ‚Ø·\n",
    "    filtered_tfidf = tfidf_matrix[[tfidf_id_to_index[doc_id] for doc_id in common_doc_ids]]\n",
    "    filtered_bert = np.array([bert_matrix[bert_id_to_index[doc_id]] for doc_id in common_doc_ids], dtype=np.float32)\n",
    "\n",
    "    # 5. Ø§Ù„ØªØ£ÙƒØ¯\n",
    "    assert filtered_tfidf.shape[0] == filtered_bert.shape[0] == len(common_doc_ids)\n",
    "\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    total_docs = len(common_doc_ids)\n",
    "\n",
    "    # 6. Ø§Ù„ØªÙ‚Ø·ÙŠØ¹ Ø¥Ù„Ù‰ Chunks\n",
    "    for start in tqdm(range(0, total_docs, chunk_size), desc=\"ğŸ§© Ù…Ø¹Ø§Ù„Ø¬Ø© Chunks\"):\n",
    "        end = min(start + chunk_size, total_docs)\n",
    "\n",
    "        tfidf_chunk = filtered_tfidf[start:end]\n",
    "        bert_chunk = filtered_bert[start:end]\n",
    "        chunk_doc_ids = common_doc_ids[start:end]\n",
    "\n",
    "        chunk_data = {\n",
    "            \"tfidf_chunk\": tfidf_chunk,\n",
    "            \"bert_chunk\": bert_chunk,\n",
    "            \"doc_ids\": chunk_doc_ids\n",
    "        }\n",
    "\n",
    "        chunk_file = os.path.join(save_path, f\"hybrid_chunk_{start}_{end}.joblib\")\n",
    "        joblib.dump(chunk_data, chunk_file, compress=3)\n",
    "\n",
    "    print(f\"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ {total_docs} ÙˆØ«ÙŠÙ‚Ø© ÙÙŠ Ø¯ÙØ¹Ø§Øª Ø¯Ø§Ø®Ù„: {save_path}\")\n",
    "\n",
    "\n"
   ],
   "id": "7d344e9dee539d9f",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T08:43:39.524268Z",
     "start_time": "2025-06-24T08:41:38.388289Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ğŸ”¸ ØªÙ†ÙÙŠØ°\n",
    "build_clean_hybrid_representation_in_chunks(\n",
    "    tfidf_path=r\"C:\\Users\\Reem Darawsheh\\Desktop\\PythonProject\\PythonProject\\Data Representation\\TF-IDF\\antique\\train\\doc\\tfidf_data.joblib\",\n",
    "    bert_path=r\"C:\\Users\\Reem Darawsheh\\Desktop\\PythonProject\\PythonProject\\Data Representation\\Bert\\antique\\train\\doc\\bert_embedding.joblib\",\n",
    "    save_path=r\"C:\\Users\\Reem Darawsheh\\Desktop\\PythonProject\\PythonProject\\Data Representation\\Hybrid\\antique\\train\\chunks\",\n",
    "    chunk_size=5000\n",
    ")"
   ],
   "id": "1eb8d518f97935ce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Ø¨Ø¯Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØªÙ…Ø«ÙŠÙ„Ø§Øª Ø§Ù„Ù‡Ø¬ÙŠÙ†Ø© Ù…Ø¹ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØªØ¯Ø§Ø®Ù„...\n",
      "ğŸ“Œ Ø¹Ø¯Ø¯ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…Ø´ØªØ±ÙƒØ©: 401768 / TF-IDF: 403666 / BERT: 401768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ§© Ù…Ø¹Ø§Ù„Ø¬Ø© Chunks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81/81 [01:09<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ 401768 ÙˆØ«ÙŠÙ‚Ø© ÙÙŠ Ø¯ÙØ¹Ø§Øª Ø¯Ø§Ø®Ù„: C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\Data Representation\\Hybrid\\antique\\train\\chunks\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:16:39.148482Z",
     "start_time": "2025-06-19T12:16:38.887994Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import joblib\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def validate_hybrid_chunk_joblib(chunk_start, chunk_end, base_path, tfidf_dim=100000, bert_dim=384):\n",
    "    # Ø¨Ù†Ø§Ø¡ Ø§Ø³Ù… Ù…Ù„Ù chunk\n",
    "    chunk_file = os.path.join(base_path, f\"hybrid_chunk_{chunk_start}_{chunk_end}.joblib\")\n",
    "\n",
    "    if not os.path.exists(chunk_file):\n",
    "        print(f\"âŒ Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {chunk_file}\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nğŸ“‚ Checking hybrid chunk joblib {chunk_start}-{chunk_end}\")\n",
    "\n",
    "    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "    chunk_data = joblib.load(chunk_file)\n",
    "\n",
    "    # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…ÙØ§ØªÙŠØ­ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©\n",
    "    required_keys = [\"tfidf_chunk\", \"bert_chunk\", \"doc_ids\"]\n",
    "    for key in required_keys:\n",
    "        if key not in chunk_data:\n",
    "            print(f\"âŒ Ø§Ù„Ù…ÙØªØ§Ø­ '{key}' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ø§Ù„Ù…Ù„Ù\")\n",
    "            return\n",
    "\n",
    "    tfidf_chunk = chunk_data[\"tfidf_chunk\"]\n",
    "    bert_chunk = chunk_data[\"bert_chunk\"]\n",
    "    doc_ids = chunk_data[\"doc_ids\"]\n",
    "\n",
    "    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯ ÙˆØ¹Ø¯Ø¯ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚\n",
    "    num_docs = len(doc_ids)\n",
    "    if tfidf_chunk.shape[0] != num_docs:\n",
    "        print(f\"âŒ Ø¹Ø¯Ø¯ Ø§Ù„ØµÙÙˆÙ ÙÙŠ tfidf_chunk ({tfidf_chunk.shape[0]}) Ù„Ø§ ÙŠØ·Ø§Ø¨Ù‚ Ø¹Ø¯Ø¯ doc_ids ({num_docs})\")\n",
    "        return\n",
    "    if bert_chunk.shape[0] != num_docs:\n",
    "        print(f\"âŒ Ø¹Ø¯Ø¯ Ø§Ù„ØµÙÙˆÙ ÙÙŠ bert_chunk ({bert_chunk.shape[0]}) Ù„Ø§ ÙŠØ·Ø§Ø¨Ù‚ Ø¹Ø¯Ø¯ doc_ids ({num_docs})\")\n",
    "        return\n",
    "    if tfidf_chunk.shape[1] != tfidf_dim:\n",
    "        print(f\"âŒ Ø¹Ø¯Ø¯ Ø£Ø¹Ù…Ø¯Ø© tfidf_chunk ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹: {tfidf_chunk.shape[1]} != {tfidf_dim}\")\n",
    "        return\n",
    "    if bert_chunk.shape[1] != bert_dim:\n",
    "        print(f\"âŒ Ø¹Ø¯Ø¯ Ø£Ø¹Ù…Ø¯Ø© bert_chunk ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹: {bert_chunk.shape[1]} != {bert_dim}\")\n",
    "        return\n",
    "\n",
    "    print(f\"âœ… Document count: {num_docs}\")\n",
    "    print(f\"âœ… TF-IDF chunk shape: {tfidf_chunk.shape}\")\n",
    "    print(f\"âœ… BERT chunk shape: {bert_chunk.shape}\")\n",
    "\n",
    "    # Ø§Ø®ØªØ¨Ø§Ø± ØªØ´Ø§Ø¨Ù‡ Ø¨Ø³ÙŠØ·: Ù†Ø­Ø³Ø¨ ØªØ´Ø§Ø¨Ù‡ cos Ø¨ÙŠÙ† Ø£ÙˆÙ„ ÙˆØ«ÙŠÙ‚ØªÙŠÙ† Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªÙ…Ø«ÙŠÙ„ Ø§Ù„Ù‡Ø¬ÙŠÙ† (concat)\n",
    "    if num_docs >= 2:\n",
    "        # Ø¬Ù…Ø¹ Ø§Ù„ØªÙ…Ø«ÙŠÙ„ Ø§Ù„Ù‡Ø¬ÙŠÙ† Ù…Ø¤Ù‚ØªÙ‹Ø§ (dense + sparse)\n",
    "        from scipy.sparse import hstack\n",
    "\n",
    "        hybrid_0 = np.hstack([tfidf_chunk[0].toarray().flatten(), bert_chunk[0]])\n",
    "        hybrid_1 = np.hstack([tfidf_chunk[1].toarray().flatten(), bert_chunk[1]])\n",
    "\n",
    "        sim = cosine_similarity([hybrid_0], [hybrid_1])[0][0]\n",
    "        print(f\"ğŸ§ª Cosine similarity Ø¨ÙŠÙ† Ø£ÙˆÙ„ ÙˆØ«ÙŠÙ‚ØªÙŠÙ† ÙÙŠ Ø§Ù„ØªÙ…Ø«ÙŠÙ„ Ø§Ù„Ù‡Ø¬ÙŠÙ†: {sim:.4f}\")\n",
    "    else:\n",
    "        print(\"â„¹ï¸ Chunk ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙˆØ«ÙŠÙ‚Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø·ØŒ Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡.\")\n",
    "\n",
    "    print(\"âœ… Ø§Ù„ØªÙ…Ø«ÙŠÙ„ Ø§Ù„Ù‡Ø¬ÙŠÙ† ÙÙŠ chunk ØµØ­ÙŠØ­.\")\n",
    "\n",
    "\n",
    "# Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…\n",
    "validate_hybrid_chunk_joblib(\n",
    "    chunk_start=0,\n",
    "    chunk_end=5000,\n",
    "    base_path=r\"C:\\Users\\Reem Darawsheh\\Desktop\\PythonProject\\PythonProject\\Data Representation\\Hybrid\\beir\\quora\\test\\chunks\",\n",
    "    tfidf_dim=102029,\n",
    "    bert_dim=384\n",
    ")\n",
    "\n"
   ],
   "id": "1dba04b4f58619fb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“‚ Checking hybrid chunk joblib 0-5000\n",
      "âœ… Document count: 5000\n",
      "âœ… TF-IDF chunk shape: (5000, 102029)\n",
      "âœ… BERT chunk shape: (5000, 384)\n",
      "ğŸ§ª Cosine similarity Ø¨ÙŠÙ† Ø£ÙˆÙ„ ÙˆØ«ÙŠÙ‚ØªÙŠÙ† ÙÙŠ Ø§Ù„ØªÙ…Ø«ÙŠÙ„ Ø§Ù„Ù‡Ø¬ÙŠÙ†: 0.9058\n",
      "âœ… Ø§Ù„ØªÙ…Ø«ÙŠÙ„ Ø§Ù„Ù‡Ø¬ÙŠÙ† ÙÙŠ chunk ØµØ­ÙŠØ­.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:17:13.379961Z",
     "start_time": "2025-06-19T12:17:13.359884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def hybrid_parallel_search(\n",
    "    query_tfidf_vector,\n",
    "    query_bert_vector,\n",
    "    chunks_path,\n",
    "    tfidf_weight=0.5,\n",
    "    bert_weight=0.5,\n",
    "    top_k=10\n",
    "):\n",
    "    \"\"\"\n",
    "    Ø¨Ø­Ø« ØªÙ…Ø«ÙŠÙ„ Ù…ØªÙˆØ§Ø²ÙŠ Ù…Ø¹ Ø¯Ù…Ø¬ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ fusion weights\n",
    "\n",
    "    - query_tfidf_vector: ØªÙ…Ø«ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… TF-IDF (sparse vector Ø£Ùˆ dense numpy array)\n",
    "    - query_bert_vector: ØªÙ…Ø«ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… BERT (dense numpy array)\n",
    "    - chunks_path: Ù…Ø¬Ù„Ø¯ ÙŠØ­ÙˆÙŠ Ù…Ù„ÙØ§Øª chunk Ø¨ØµÙŠØºØ© joblib Ù…Ø¹ ØªÙ…Ø«ÙŠÙ„Ø§Øª TF-IDF Ùˆ BERT\n",
    "    - tfidf_weight, bert_weight: Ø£ÙˆØ²Ø§Ù† Ø¯Ù…Ø¬ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ (ÙŠØ¬Ø¨ Ø£Ù† Ù…Ø¬Ù…ÙˆØ¹Ù‡Ù…=1)\n",
    "    - top_k: Ø¹Ø¯Ø¯ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ø§Ù„Ù…Ø±Ø§Ø¯ Ø¥Ø±Ø¬Ø§Ø¹Ù‡Ø§\n",
    "    \"\"\"\n",
    "\n",
    "    assert abs(tfidf_weight + bert_weight - 1.0) < 1e-6, \"Ù…Ø¬Ù…ÙˆØ¹ Ø§Ù„Ø£ÙˆØ²Ø§Ù† ÙŠØ¬Ø¨ Ø£Ù† ÙŠØ³Ø§ÙˆÙŠ 1\"\n",
    "\n",
    "    final_results = []\n",
    "\n",
    "    chunk_files = sorted([f for f in os.listdir(chunks_path) if f.endswith(\".joblib\")])\n",
    "    for chunk_file in tqdm(chunk_files, desc=\"âš™ï¸ Ù…Ø¹Ø§Ù„Ø¬Ø© chunks\"):\n",
    "        chunk_data = joblib.load(os.path.join(chunks_path, chunk_file))\n",
    "\n",
    "        tfidf_chunk = chunk_data[\"tfidf_chunk\"]  # sparse matrix\n",
    "        bert_chunk = chunk_data[\"bert_chunk\"]    # dense numpy array\n",
    "        doc_ids = chunk_data[\"doc_ids\"]\n",
    "\n",
    "        # Ø­Ø³Ø§Ø¨ ØªØ´Ø§Ø¨Ù‡ TF-IDF (cosine similarity)\n",
    "        sims_tfidf = cosine_similarity(query_tfidf_vector, tfidf_chunk).flatten()\n",
    "\n",
    "        # Ø­Ø³Ø§Ø¨ ØªØ´Ø§Ø¨Ù‡ BERT (cosine similarity)\n",
    "        sims_bert = cosine_similarity([query_bert_vector], bert_chunk).flatten()\n",
    "\n",
    "        # Ø¯Ù…Ø¬ Ø§Ù„ØªØ´Ø§Ø¨Ù‡Ø§Øª Ø¨Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©\n",
    "        sims_fused = tfidf_weight * sims_tfidf + bert_weight * sims_bert\n",
    "\n",
    "        # Ø¬Ù…Ø¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù…Ø¹ doc_ids\n",
    "        final_results.extend(zip(doc_ids, sims_fused))\n",
    "\n",
    "    # ØªØ±ØªÙŠØ¨ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© ØªÙ†Ø§Ø²Ù„ÙŠÙ‹Ø§ Ø­Ø³Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡\n",
    "    final_results.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_results = final_results[:top_k]\n",
    "\n",
    "    print(\"\\nâœ… Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© (Top {}):\".format(top_k))\n",
    "    for i, (doc_id, score) in enumerate(top_results, 1):\n",
    "        print(f\"{i}. {doc_id} | Score: {score:.4f}\")\n",
    "\n",
    "    return top_results\n"
   ],
   "id": "c8044b0195a249e0",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:43:46.747506Z",
     "start_time": "2025-06-19T12:43:24.791958Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "# 1. Ø­Ø¶Ù‘Ø± Ù…ÙˆØ¯ÙŠÙ„ BERT\n",
    "bert_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# 2. Ø¬Ù‡Ø² Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…\n",
    "query = \"how to learn a amchine programming language? \"\n",
    "\n",
    "# 3. Ø­Ù…Ù‘Ù„ vectorizer Ù…Ù† Ù…Ù„Ù tfidf_data.joblib (Ù„Ø£Ù†Ù‡ ÙŠØ­ØªÙˆÙŠÙ‡)\n",
    "vectorizer_data = joblib.load(r\"C:\\Users\\Reem Darawsheh\\Desktop\\PythonProject\\PythonProject\\Data Representation\\TF-IDF\\beir\\quora\\test\\doc\\tfidf_data.joblib\")\n",
    "vectorizer = vectorizer_data[\"vectorizer\"]\n",
    "\n",
    "# 4. Ø­ÙˆÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¥Ù„Ù‰ ØªÙ…Ø«ÙŠÙ„ TF-IDF\n",
    "query_tfidf_vector = vectorizer.transform([query])\n",
    "\n",
    "# 5. ØªÙ…Ø«ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… BERT\n",
    "query_bert_vector = bert_model.encode(query, convert_to_numpy=True).astype(np.float32)\n",
    "\n",
    "# 6. Ù†ÙÙ‘Ø° Ø§Ù„Ø¨Ø­Ø« (ØªØ£ÙƒØ¯ Ø£Ù† ØªØ§Ø¨Ø¹ hybrid_parallel_search Ù…Ø¹Ø±Ù ÙˆÙ…ØªØ§Ø­)\n",
    "results = hybrid_parallel_search(\n",
    "    query_tfidf_vector=query_tfidf_vector,\n",
    "    query_bert_vector=query_bert_vector,\n",
    "    chunks_path=r\"C:\\Users\\Reem Darawsheh\\Desktop\\PythonProject\\PythonProject\\Data Representation\\Hybrid\\beir\\quora\\test\\chunks\",\n",
    "    tfidf_weight=0.4,\n",
    "    bert_weight=0.6,\n",
    "    top_k=10\n",
    ")\n",
    "\n",
    "# 7. Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬\n",
    "print(results)\n"
   ],
   "id": "7a28d69808c56424",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "âš™ï¸ Ù…Ø¹Ø§Ù„Ø¬Ø© chunks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:16<00:00,  6.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© (Top 10):\n",
      "1. 91642 | Score: 0.6005\n",
      "2. 527808 | Score: 0.5991\n",
      "3. 498101 | Score: 0.5965\n",
      "4. 44161 | Score: 0.5810\n",
      "5. 1271 | Score: 0.5602\n",
      "6. 112819 | Score: 0.5481\n",
      "7. 364397 | Score: 0.5396\n",
      "8. 22650 | Score: 0.5242\n",
      "9. 9811 | Score: 0.5242\n",
      "10. 419330 | Score: 0.5234\n",
      "[('91642', np.float64(0.6005096524953842)), ('527808', np.float64(0.5991446912288666)), ('498101', np.float64(0.5965242475271225)), ('44161', np.float64(0.5810033917427063)), ('1271', np.float64(0.5602440029382706)), ('112819', np.float64(0.5480646084057077)), ('364397', np.float64(0.5395946989761575)), ('22650', np.float64(0.524203137870819)), ('9811', np.float64(0.524203137870819)), ('419330', np.float64(0.5233781973825677))]\n"
     ]
    }
   ],
   "execution_count": 21
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
