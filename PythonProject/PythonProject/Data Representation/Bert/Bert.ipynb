{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T16:52:58.153754Z",
     "start_time": "2025-06-23T14:18:31.569897Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import string\n",
    "import torch\n",
    "import joblib\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import ir_datasets\n",
    "\n",
    "# تحميل الموارد\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# الإعدادات\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "punctuations = set(string.punctuation)\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# تحميل BERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# دالة تنظيف\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    cleaned_tokens = []\n",
    "    for token in tokens:\n",
    "        if token in punctuations or token in stop_words or token.isdigit():\n",
    "            continue\n",
    "        lemma = lemmatizer.lemmatize(token)\n",
    "        if len(lemma) < 3:\n",
    "            continue\n",
    "        cleaned_tokens.append(lemma)\n",
    "    return ' '.join(cleaned_tokens)\n",
    "\n",
    "# دالة تمثيل BERT\n",
    "def get_bert_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "    return embeddings.squeeze().cpu().numpy()\n",
    "\n",
    "# المسار والداتاسيت\n",
    "dataset_name = \"antique/train\"\n",
    "dataset = ir_datasets.load(dataset_name)\n",
    "save_path = os.path.join(dataset_name.replace(\"/\", os.sep), \"doc\")\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "save_file = os.path.join(save_path, \"bert_embedding.joblib\")\n",
    "\n",
    "print(f\"🚀 Loading and processing dataset: {dataset_name}\")\n",
    "\n",
    "doc_ids = []\n",
    "embeddings = []\n",
    "\n",
    "for doc in tqdm(dataset.docs_iter(), total=dataset.docs_count(), desc=\"🔄 Processing\"):\n",
    "    clean_text = preprocess(doc.text)\n",
    "    if not clean_text.strip():\n",
    "        continue\n",
    "    emb = get_bert_embedding(clean_text)\n",
    "    doc_ids.append(doc.doc_id)\n",
    "    embeddings.append(emb)\n",
    "\n",
    "# حفظ البيانات\n",
    "embedding_data = {\n",
    "    \"doc_ids\": doc_ids,\n",
    "    \"embeddings_matrix\": embeddings,\n",
    "    \"model_name\": MODEL_NAME\n",
    "}\n",
    "joblib.dump(embedding_data, save_file)\n",
    "\n",
    "print(f\"✅ BERT embeddings saved to: {save_file}\")\n"
   ],
   "id": "e05557dbfb5294e8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Azzam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Azzam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Azzam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Azzam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Loading and processing dataset: antique/train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔄 Processing: 100%|██████████| 403666/403666 [2:30:56<00:00, 44.57it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ BERT embeddings saved to: antique\\train\\doc\\bert_embedding.joblib\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T20:26:24.632498Z",
     "start_time": "2025-06-18T18:24:09.800340Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\Reem Darawsheh\\Desktop\\PythonProject\\PythonProject\\scripts\")  # المسار الذي يحتوي على bert_embedder.py\n",
    "\n",
    "from bert_embedder import process_bert_from_mongodb\n",
    "\n",
    "\n",
    "\n",
    "# Quora Dataset\n",
    "process_bert_from_mongodb(\"beir/quora/test\", \"documents_quora_test\")"
   ],
   "id": "c9fb43223f756cea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing BERT embeddings from MongoDB collection: documents_quora_test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 522931/522931 [1:59:47<00:00, 72.76it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ BERT embeddings saved to: beir\\quora\\test\\doc\\bert_embedding.joblib\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T15:11:13.601409Z",
     "start_time": "2025-06-22T15:11:12.551487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "collection = client[\"ir_project\"][\"documents_test\"]\n",
    "\n",
    "print(\"عدد المستندات:\", collection.count_documents({}))\n"
   ],
   "id": "246bf980cd7634d0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "عدد المستندات: 403666\n"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
