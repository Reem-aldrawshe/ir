{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T16:52:58.153754Z",
     "start_time": "2025-06-23T14:18:31.569897Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import string\n",
    "import torch\n",
    "import joblib\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import ir_datasets\n",
    "\n",
    "# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "punctuations = set(string.punctuation)\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# ØªØ­Ù…ÙŠÙ„ BERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Ø¯Ø§Ù„Ø© ØªÙ†Ø¸ÙŠÙ\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    cleaned_tokens = []\n",
    "    for token in tokens:\n",
    "        if token in punctuations or token in stop_words or token.isdigit():\n",
    "            continue\n",
    "        lemma = lemmatizer.lemmatize(token)\n",
    "        if len(lemma) < 3:\n",
    "            continue\n",
    "        cleaned_tokens.append(lemma)\n",
    "    return ' '.join(cleaned_tokens)\n",
    "\n",
    "# Ø¯Ø§Ù„Ø© ØªÙ…Ø«ÙŠÙ„ BERT\n",
    "def get_bert_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "    return embeddings.squeeze().cpu().numpy()\n",
    "\n",
    "# Ø§Ù„Ù…Ø³Ø§Ø± ÙˆØ§Ù„Ø¯Ø§ØªØ§Ø³ÙŠØª\n",
    "dataset_name = \"antique/train\"\n",
    "dataset = ir_datasets.load(dataset_name)\n",
    "save_path = os.path.join(dataset_name.replace(\"/\", os.sep), \"doc\")\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "save_file = os.path.join(save_path, \"bert_embedding.joblib\")\n",
    "\n",
    "print(f\"ğŸš€ Loading and processing dataset: {dataset_name}\")\n",
    "\n",
    "doc_ids = []\n",
    "embeddings = []\n",
    "\n",
    "for doc in tqdm(dataset.docs_iter(), total=dataset.docs_count(), desc=\"ğŸ”„ Processing\"):\n",
    "    clean_text = preprocess(doc.text)\n",
    "    if not clean_text.strip():\n",
    "        continue\n",
    "    emb = get_bert_embedding(clean_text)\n",
    "    doc_ids.append(doc.doc_id)\n",
    "    embeddings.append(emb)\n",
    "\n",
    "# Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "embedding_data = {\n",
    "    \"doc_ids\": doc_ids,\n",
    "    \"embeddings_matrix\": embeddings,\n",
    "    \"model_name\": MODEL_NAME\n",
    "}\n",
    "joblib.dump(embedding_data, save_file)\n",
    "\n",
    "print(f\"âœ… BERT embeddings saved to: {save_file}\")\n"
   ],
   "id": "e05557dbfb5294e8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Azzam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Azzam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Azzam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Azzam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Loading and processing dataset: antique/train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Processing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 403666/403666 [2:30:56<00:00, 44.57it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… BERT embeddings saved to: antique\\train\\doc\\bert_embedding.joblib\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T20:26:24.632498Z",
     "start_time": "2025-06-18T18:24:09.800340Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\Reem Darawsheh\\Desktop\\PythonProject\\PythonProject\\scripts\")  # Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø°ÙŠ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ bert_embedder.py\n",
    "\n",
    "from bert_embedder import process_bert_from_mongodb\n",
    "\n",
    "\n",
    "\n",
    "# Quora Dataset\n",
    "process_bert_from_mongodb(\"beir/quora/test\", \"documents_quora_test\")"
   ],
   "id": "c9fb43223f756cea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Processing BERT embeddings from MongoDB collection: documents_quora_test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 522931/522931 [1:59:47<00:00, 72.76it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… BERT embeddings saved to: beir\\quora\\test\\doc\\bert_embedding.joblib\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T15:11:13.601409Z",
     "start_time": "2025-06-22T15:11:12.551487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "collection = client[\"ir_project\"][\"documents_test\"]\n",
    "\n",
    "print(\"Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª:\", collection.count_documents({}))\n"
   ],
   "id": "246bf980cd7634d0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª: 403666\n"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
